\documentclass[a4paper,11pt]{scrartcl}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{ngerman}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{listings}
\usepackage{color}
\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage{amssymb} % mathbb
\usepackage{amsmath}% http://ctan.org/pkg/amsmath
\usepackage{hhline}

\newcommand{\zB}{\mbox{z.\,B.}\xspace}
\newcommand{\bspw}{\mbox{bspw.}\xspace}
\newcommand{\bzw}{\mbox{bzw.}\xspace}
\newcommand{\iAllg}{\mbox{i.\,Allg.}\xspace}
\newcommand{\ua}{\mbox{u.\,a.}\xspace}
\newcommand{\vs}{\mbox{vs.}\xspace}
\newcommand{\inkl}{\mbox{inkl.}\xspace}

\def\CC{{C\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\tiny\bf ++}}}
\def\GPP{{G\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\tiny\bf ++}}}
\setlength{\parindent}{0em} % Einrückung verhindern

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

% \fontfamily{pzc}\selectfont
% \ttfamily

\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}; should come as last argument
  basicstyle=\footnotesize\fontfamily{bch}\selectfont,        
  % the size & type of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  deletekeywords={min,max},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,	                   % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  keywordstyle=\color{blue},       % keyword style
  language=C,                 % the language of the code
  morekeywords={},                 % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=1,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=2,	                   % sets default tabsize to 2 spaces
%   title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  
\title{\includegraphics[width=0.6\textwidth]{bilder/tuc-logo-black.pdf}
\\Effiziente Implementierung von Matrix-Algorithmen für Multicore-Systeme
}
\subtitle{Praktikum Forschungsschwerpunkt Parallele und verteilte Systeme}
\author{Autor: Matthias Tietz}


\begin{document}

\maketitle \thispagestyle{empty}

\newpage
%%% Informationen/Leerseite %%%
\thispagestyle{empty}
~
\vfill
Technische Universität Chemnitz\\
Fakultät für Informatik\\
Professur Praktische Informatik\\
Praktikum Forschungsschwerpunkt Parallele und verteilte Systeme\\
Wintersemester 2016/2017\\

Effiziente Implementierung von Matrix-Algorithmen für Multicore-Systeme\\
Autor: Matthias Tietz\\
Matrikelnummer:~375681\\
Bachelor Informatik, 5.~Fachsemester

\newpage \tableofcontents
\newpage

\section{Einleitung}\label{chp:Einleitung}
Matrix-Algorithmen finden in einer Vielzahl verschiedener Bereiche Anwendung.
So werden \bspw in der linearen Algebra häufig Matrizen eingesetzt, um lineare Gleichungssysteme oder Eigenwertprobleme zu lösen. Im Kontext linearer Abbildungen lassen sich geometrische Transformationen
durch Matrizenprodukte abbilden, was als Grundlage für die Computergrafik zur Realisierung von Koordinatentransformationen dient.\newline

Der naive Standardalgorithmus zur Matrixmultiplikation 
$C\,= A \cdot B$ mit $A \in \mathbb{R}^{l \times m}$, $B \in \mathbb{R}^{m \times n}$,
$C \in \mathbb{R}^{l \times n}$ verfügt über eine kubische Laufzeit, für jedes Element $c$ der 
Ergebnismatrix $C$ müssen die Werte eines Zeilenvektors von $A$ mit den Werten eines Spaltenvektors
von $B$ schrittweise multipliziert und in $c$ aufsummiert werden. Die Ergebnismatrix $C$
hat die Dimensionen $l \times n$ und für jedes Element aus $C$ entsteht ein linearer Aufwand
$m \Rightarrow$ $\mathcal{O}(m \cdot l \cdot n)$.\newline

Der Algorithmus ist also einfach aufgebaut, es finden nur mathematisch grundlegende Operationen
(Multiplikation, Addition) statt, jedoch in einer großen Anzahl. 
Des Weiteren müssen die Werte aus den Ausgangsmatrizen mehrfach verwendet werden, um \bspw die erste Zeile der Ergebnismatrix $C$ zu berechnen,
benötigen wir für jedes einzelne Element $c$ den selben Zeilenvektor aus $A$, das gilt für die
Spaltenvektoren von $B$ analog.
Wenn die Datenzugriffe, wie beim Standardalgorithmus, also rein iterativ stattfinden,
wird das Speicherverhalten ignoriert und damit \emph{Cache Misses}\footnote{Die zu lesenden oder
schreibenden Daten liegen nicht im Cache, daher muss auf den Hauptspeicher zugegriffen werden.
$\rightarrow$ größere Latenz}
 in Kauf genommen. \newline


Aus den genannten Gründen ergibt sich das Potenzial, das Vorgehen bei der Matrixmultiplikation
und die daraus resultierende Performance zu verbessern. So könnte man die \texttt{for()}-Schleifen so umstrukturieren, dass man diese in kleinere
Iterationsblöcke aufteilt und damit vom Lokalitätsprinzip Gebrauch macht. Ein weiterer Ansatz wäre
ein paralleles Verfahren, um den Zeitaufwand zu minimieren, in dem mehrere Operationen in einem
Schritt ausgeführt werden.

\section{Implementierung}
Bei der Umsetzung der Matrix-Algorithmen ist in erster Linie die Performance-Verbesserung 
von Bedeutung, daher wurde bei den eigenen Implementierungen ausschließlich die Sprache 
\texttt{C} eingesetzt. Von Interesse sind die reinen Algorithmen (Funktionen), es wurde daher
kaum Wert auf Benutzerfreundlichkeit, umfassende Parameterprüfung oder Tests gelegt --
man geht davon aus, dass der Nutzer korrekte Eingabewerte verwendet.\newline

Zu Beginn der Implementierung hatte ich für die Matrizen einen Datentyp als
\texttt{struct} definiert, was sich aber als unnötig herausgestellt hat, der
Code sollte auf das Notwendige reduziert sein. Daher werden die einzelnen 
Matrizen einfach als float-Array umgesetzt. Zur Vereinfachung der verschiedenen
Verfahren wurde eine Einschränkung auf quadratische Matrizen ($n \times n$) 
und Parametergrößen (Dimension, Blockgröße) als Zweierpotenzen vorgenommen.\newline

Mit dem Ziel der Optimierungen, wird auch bewusst auf einen guten Coding-Stil verzichtet,
so werden in den performancerelevanten Bereichen unnötige Funktionsaufrufe oder Variablen vermieden, was einen schlechter lesbaren Code zur Folge hat. So wird \bspw anstatt einer
Hilfsfunktion zum Setzen eines Wertes in einer bestimmten Matrix

\lstinputlisting[language=C]{code/set.c}
direkt \texttt{result[(N * i) + j] = value;} verwendet.\newline

Die Ausgangsmatrizen $A$ und $B$
werden vor Benutzung in den verschiedenen Verfahren mit pseudo-zufälligen Werten initialisiert
(\texttt{float$*$ createRandomizedMatrix\_f(int N);}), da die konkreten Werte nicht von großer Bedeutung sind, sollte dieses Vorgehen ausreichend sein. Die Ergebnismatrizen werden außerhalb des eigentlichen
Algorithmus mit Nullen initialisiert (\texttt{calloc()}). Das Repository \inkl einer kleinen
Beschreibung ist auf GitHub \cite{ghub} frei verfügbar.


\subsection{Standardalgorithmus}
Der Standardalgorithmus stellt die klassische (naive) Variante der Matrixmultiplikation
dar, bei der die Ausführung uneingeschränkt über die komplette Dimension $N$ der einzelnen Matrizen 
stattfindet. Die Funktion \texttt{standardMatrixMul\_f()} erwartet die Eingabematrizen\footnote{
Eindimensionales float-Array der Größe \texttt{N$\times$N}} \texttt{a} und \texttt{b},
die Matrix \texttt{result}, welche schrittweise die Ergebnisse der Berechnungen speichert, 
und die Dimension \texttt{N}.\newline

\lstinputlisting[language=C]{code/std.c}


In den beiden äußeren Schleifen 
wird über die Zeilen und Spalten der Ergebnismatrix \texttt{result} iteriert,
in der innersten Schleife wird dann die Produktsumme der Eingangsmatrizen in der
Variable \texttt{calc} gebildet und der Wert anschließend gespeichert.
Die einzige Optimierung ist hierbei der direkte Zugriff per Index auf die Matrizen.


\subsection{Cache-optimiertes Verfahren}
Bei dem Cache-optimierten Verfahren fand die Umsetzung mittels \emph{Loop tiling} statt,
wodurch die Ausführung der Schleifen effizienter wird. Das Loop tiling gliedert die 
Matrizen in kleinere Blöcke, auf denen die Matrixmultiplikation durchgeführt wird.
Dadurch verbleiben Daten, die bald wieder verwendet werden, im schnellen Hauptspeicher der 
CPU (Cache). Damit addressiert man die Problematik der Speicherzugriffe, da gerade sehr große
Matrizen nicht komplett im Cache gehalten werden können.

\lstinputlisting[language=C]{code/block.c}

Die Funktionssignatur besteht nun neben den bekannten Matrizen \texttt{a, b, result}
und der Dimension \texttt{N} nun noch zusätzlich aus einem Parameter \texttt{BS} für 
die Blockgröße, in die die Matrizen unterteilt werden sollen.\newline

In den äußeren drei Schleifen (Zeile 4-6) wird dafür gesorgt, dass der korrekte Offset eingehalten wird,
sodass, anstatt direkt über die komplette Dimension \texttt{N} der Matrizen zu iterieren, nur auf den
kleinen Blöcken der Größe \texttt{BLOCK} gearbeitet wird. Daher gilt zu beachten, dass die Indexvariablen
\texttt{i,~j} und \texttt{k} nicht einfach inkrementiert werden wie bei herkömmlichen Schleifen,
sondern um den Offset-Wert \texttt{BLOCK} erhöht werden. Daher auch die Forderung, dass 
die Parametergrößen als Zweierpotenz zu wählen sind, sodass $N \bmod BS = 0$, sich also jede 
Matrix restlos in Blöcke unterteilen lässt.\newline

Die inneren Schleifen (Zeile 8-12) führen dann für die einzelnen Blöcke eine reguläre 
Matrixmultiplikation aus. Hier bekommen die inneren Schleifenvariablen \texttt{y, x, z} den korrekten 
Startpunkt aus \texttt{i, j, k} übergeben und durchlaufen dann anhand der jeweiligen 
Schleifenabbruchbedingung die Berechnungen auf Matrizen der Größe \texttt{BLOCK} $\times$ \texttt{BLOCK}.

\subsection{Paralleles Verfahren}
Wie bereits in der Einleitung~\ref{chp:Einleitung} erwähnt, ist für die 
Matrizenmultiplikation eine große Anzahl gleichartiger Rechenoperationen notwendig.
Daher ergibt sich die Möglichkeit, diese Eigenschaft der Parallelisierbarkeit auszunutzen,
also mehrere Daten quasi-parallel zu verarbeiten.\newline

Um dieses Vorhaben umzusetzen, bieten sich die Advanced Vector Extensions (AVX) an,
welche eine Befehlssatz-Erweiterung für Mikroprozessoren darstellen. Diese Prozessoren
verfügen über SIMD-Erweiterungen (Register, Instruktionen), die es ermöglichen, mit einem Befehlsaufruf  mehrere einheitliche Datensätze parallel zu verarbeiten. Um diese Funktionalitäten auf 
Hochsprachen-Ebene zu verwenden, können mithilfe von intrinsischen Funktionen 
die prozessorspezifischen Befehle als normale Funktionsaufrufe abstrahiert werden,
Unterstützung dafür bieten verschiedene Compiler. Eine gute Übersicht über die sogenannten
\emph{Intrinsics} findet man auf der Website von Intel \cite{intr}.\newline

Die relevanten Funktionen, um die Matrixmultiplikation mithilfe von AVX umzusetzen,
umfassen mindestens die Möglichkeiten Werte aus einer Matrix laden/speichern
und Additionen/Multiplikationen ausführen zu können. Da die bisherigen Matrizen auch auf 
float-Werten basieren, kommen für die Berechnungen float-Vektoren (\texttt{\_\_m256}) zum
Einsatz, welche eine Kapazität von 256 Bit bereitstellen und damit 8 floats speichern
können. Hier eine Übersicht über die in meiner Implementierung benutzten Funktionen:\newline

\begin{tabular}{| l | l |}
\hline
Name & Beschreibung \\ \hline
  
\texttt{void \textbf{\_mm256\_storeu\_ps}} & speichert einen 256-bit-Vektor \texttt{a} an der Stelle\\
\texttt{(float$*$ mem\_addr, \_\_m256 a)} &  \texttt{mem\_addr} ab   \\ \hline

\texttt{\_\_m256 \textbf{\_mm256\_add\_ps}} & addiert die Elemente der Vektoren \texttt{a} und 
\texttt{b} \\
\texttt{(\_\_m256 a, \_\_m256 b)} & und gibt den Ergebnisvektor zurück \\ \hline

\texttt{\_\_m256 \textbf{\_mm256\_mul\_ps}} & multipliziert die Elemente der Vektoren \texttt{a} und \texttt{b} \\
\texttt{(\_\_m256 a, \_\_m256 b)} & und gibt den Ergebnisvektor zurück \\ \hline

\texttt{\_\_m256 \textbf{\_mm256\_set1\_ps} (float a)} & erstellt einen Vektor, dessen Elemente alle den 
\\
& Wert \texttt{a} erhalten \\ \hline

\texttt{\_\_m256 \textbf{\_mm256\_loadu\_ps}} & lädt die Werte aus \texttt{mem\_addr} in einen Vektor und \\
\texttt{(float const$*$ mem\_addr)} & gibt diesen zurück \\ \hline
\end{tabular}\\

Durch Einbinden des Headers \texttt{<immintrin.h>} stehen alle AVX-Funktionen bereit.


% Implementierung
\lstinputlisting[language=C]{code/avx.c}
In der Funktionssignatur tauchen wieder die drei bekannten Matrizen und die Dimension \texttt{N} auf.
Für die Größe der Vektoren im AVX-Verfahren wurde eine globale Variable \texttt{AVX\_VECTOR\_SIZE} ($=8$)
eingeführt, der Algorithmus für dieses Verfahren richtet sich direkt nach dieser Größeneinteilung.
AVX bietet die Datenstruktur der Vektoren (\texttt{\_\_m256}, single-precision) und dafür definierte Funktionen an, wodurch (Rechen-)Operationen auf 8 Datensätzen gleichzeitig vollzogen werden können.
\newline

Die äußerste Schleife (Zeile 4) dient zur Iteration über die Zeilen der Matrix \texttt{a}
und zur Bestimmung der Position zum Speichern des Ergebnisvektors in \texttt{result}.
Um die Anzahl der notwendigen Zwischenberechnungen abzuspeichern, dient die Variable
\texttt{calculation\_count}. Die Matrix \texttt{b} hat eine Spaltenzahl von \texttt{N},
so lässt sich diese durch die Vektorlänge in \texttt{N}$/8$ einteilen, da die Werte aus 
$8$ verschiedenen Spalten in einem Vektor abgelegt werden.\newline

Die Schleife in Zeile 7 iteriert nun über diese Einteilung und bestimmt die finale Position zum 
Speichern des Ergebnisvektors. In der innersten Schleife (Zeile 11) findet nun die eigentliche
Berechnung mittels AVX statt. Hier wird für Matrix \texttt{a} aus jeder einzelnen Spalte
\texttt{k} (Einteilung \texttt{N})
aus Zeile \texttt{i} ein broadcast-Vektor\footnote{\texttt{\_\_m256 \_mm256\_set1\_ps (float a)}}
erzeugt, da dieser eine float-Wert sowieso mit mehreren Werten aus Matrix \texttt{b} 
(Zeile \texttt{k}) multipliziert werden muss, daher wird das direkt in einem Schritt erledigt
und das Ergebnis anschließend an der Stelle \texttt{result\_address} aufsummiert.\newline

Die Umsetzung der Ergebnis-Speicherung in \texttt{result} bietet noch Optimierungspotenzial,
denn es muss nach jeder Vektormultiplikation, der aktuelle Vektor von \texttt{result\_address}
geladen, auf den Ergebnisvektor aufaddiert und das Ergebnis der Addition wieder an der
ursprünglichen Stelle gespeichert werden. Es sind also drei AVX-Rufe notwendig um 
"`\texttt{+=}"' zu realisieren. \newline

Um weniger Overhead zu verursachen, wurde in den Zeilen 12-17 bewusst auf das explizite Speichern von 
Zwischenergebnissen verzichtet, stattdessen wurden die Rückgaben der Funktionsaufrufe direkt
für die übergeordnete Funktion verwendet. Eine serielle und besser verständliche Notation der
Berechnungsvorschrift sieht etwa so aus:\newline

\lstinputlisting[language=C]{code/avx_s.c}


\subsection{BLAS}
Zu Vergleichszwecken sollte eine beliebige BLAS-Softwarebibliothek verwendet werden, welche 
eine Spezifikation für grundlegende Operationen der linearen Algebra, \ua die Matrixmultiplikation, darstellt. ATLAS \cite{atlas} ist eine hochoptimierte BLAS-Implementation und wurde im 
Rahmen dieses Praktikums eingesetzt. Nach erfolgreicher
Installation und Einbinden des Headers \texttt{<cblas.h>} ist die Schnittstelle zu BLAS
verfügbar.\newline

ATLAS wurde mit Fokus auf eine sehr gute Performance entworfen, zentraler Punkt ist hierbei die
automatisch-computergestützte Optimierung\footnote{Automated Empirical Optimization of Software (AEOS)}.
So werden relevante Parameter an ein konkretes System angepasst und aus einer Anzahl verschiedener Funktions-Implementierungen eine geeignete gewählt. Außerdem kann mittels \emph{Code generation}
Programmcode erzeugt werden, der eine besonders gute Performance auf dem System erreicht.\newline

Um nun wie bei den anderen Verfahren eine Matrixmultiplikation auf float-Werten auszuführen, 
bietet die Bibliothek den Funktionsruf \texttt{cblas\_sgemm(…)} (Single Precision, General Matrix Multiplication) an, wodurch allgemein die Gleichung  $C = \alpha \cdot op(A) \cdot op(B) + 
\beta \cdot C$ \cite{blasqr} umgesetzt wird. Durch Deaktivieren der Operationen $op(X)$ auf den 
Eingabematrizen und Setzen von $\alpha = 1, \beta = 0$ erhält man die gewöhnliche Matrixmultiplikation 
der Form $C\,= A \cdot B$.

\section{Benchmark}
%bezug main ... wiederholungen, etc

Das Benchmarkprogramm wurde im Rahmen der \texttt{main()}-Funktion umgesetzt,
der Code ist so aufgebaut, dass jedes Verfahren mehrmals (R) ausgeführt 
und aus der benötigten Zeit der Durchschnitt gebildet wird. Die notwendigen Parameter
(Matrixgröße, Blockgröße, Wiederholungsanzahl) müssen direkt über die Kommandozeile übergeben werden.
Eine Ausführung mit $N = 1024, BS = 8, R = 5$ sieht dann so aus: \texttt{./a.out 1024 8 5}.
\newline

Um sicherzustellen, dass die Berechnungen der einzelnen Verfahren das gleiche Ergebnis
liefern, wird die Hilfsfunktion \texttt{compareResultMatrices\_f()} eingesetzt, die
die Werte der konkreten Ergebnismatrix mit denen vom Standardalgorithmus vergleicht.
Hier noch einmal der Hinweis, dass primär der Nutzer für korrekte Eingaben verantwortlich ist, 
grob falsche numerische Werte werden jedoch abgefangen.
Nach erfolgreicher Ausführung aller Verfahren ist dann im Ordner \texttt{result/} eine Textdatei (Name
siehe Konsole) mit den Ergebnissen und zugehörigen Parametern verfügbar. \newline

Als Testsystem kam ein Intel i5-3570K $@$ 3.40GHz zum Einsatz, bei der
Kompilierung des Programms mit dem \texttt{gcc} wurde das Optimierungslevel \texttt{O2} verwendet.
Jedes Verfahren wird für die unterschiedlichen
Parameter $10 \times$ ausgeführt und aus der benötigten Zeit wurde dann der
Durchschnitt gebildet.


\subsection{Performancemessung und -vergleich}\label{chp:Bench1}
Es folgen nun die konkreten Messwerte als tabellarische Übersicht.
Für das Cache-optimierte Verfahren wurde jeweils eine feste Blockgröße von $8$ gewählt,
da sich mit diesem Wert eine günstige Rechenzeit ergeben hat.
In der ersten Spalte ist die jeweils getestete Matrixgröße \texttt{N} abgebildet, 
die restlichen Spalten beinhalten die durchschnittlich benötigte Zeit und den Speedup des jeweiligen
Verfahrens in Relation zum Standardalgorithmus.

\begin{center}
\begin{tabular}{| l | l || l || l || l |}
\hline
N $\diagdown$ Verf. & Std.algor. & Cache-opt. & Parallel (AVX) & BLAS  \\ \hline

64 & 0.474 ms & 0.534 ms | 0.89$\times$ & 0.177 ms | 2.68$\times$ & 0.092 ms | 5.15$\times$  \\ \hline
128 & 3.962 ms & 3.418 ms | 1.16$\times$ & 1.186 ms | 3.34$\times$ & 0.427 ms | 9.27$\times$  \\ \hline 
\hline
512 & 0.199 s & 0.129 s | 1.54$\times$ & 0.068 s | 2.94$\times$ & 0.013 s | 15.9$\times$  \\ \hline
1024 & 2.078 s &  1.074 s | 1.94$\times$ & 0.670 s | 3.10$\times$ & 0.094 s | 21.9$\times$ \\ \hline
2048 & 53.91 s & 10.10 s | 5.34$\times$ & 10.48 s | 5.14$\times$ & 0.743 s | 72.5$\times$ \\ \hline
\end{tabular}
\end{center}

Wie zu erwarten liefert der Standardalgorithmus \iAllg die schlechteste Laufzeit aller 
Verfahren und die BLAS-Bibliothek deutlich die beste Performance.
Eine Ausnahme gilt für $N=64$, dort ist die langsamste Technik der Cache-optimierte
Algorithmus, was wohl daran liegt, dass für solch kleine Matrizen das Loop tiling mehr
Overhead als tatsächlichen Nutzen erbringt.\newline

Der Algorithmus mittels AVX ist für kleine und mittelgroße ($64-512$) Matrizen sehr gut
geeignet, im Vergleich zu Verfahren 1 und 2 werden deutlich bessere Zeiten erzielt
($\approx 3\times$).
Das Cache-optimierte Verfahren verbessert die benötigte Laufzeit mit wachsendem \texttt{N}
und erreicht bei $N=2048$ ein besseres Ergebnis als AVX.Für kleine Matrizen ist zumindest das AVX-Verfahren noch in grober Nähe von BLAS,
jedoch verbessert sich der BLAS-Speedup mit steigender Matrixgröße im Verhältnis
zu Verfahren 2/3 sehr stark, das Optimierungspotenzial nimmt wohl auch mit der Matrixgröße zu.
\newline

Es wurden keine $N>2048$ betrachtet, da bereits für $N=2048$
der Standardalgorithmus eine relativ lange Rechenzeit benötigt, vor allem wenn weiterhin mehrere
Durchläufe zum Mitteln des Ergebnisses eingesetzt werden.


\subsection{Einfluss der Blockgröße}\label{chp:BS}
Für das Cache-optimierte Verfahren sollte der Einfluss der Blockgröße auf die Laufzeit
untersucht werden. Grundsätzlich machen natürlich nur Blockgrößen (BS): $BS \leq N/2$
Sinn, damit die Matrizen auch tatsächlich in kleinere Blöcke unterteilt werden.
Die Tabelle zeigt die Messwerte für verschiedene Matrixgrößen \texttt{N} (erste Spalte)
und mehrere \texttt{BS} (restliche Spalten), für die gilt: $BS \ll N$.

\begin{center}
\begin{tabular}{| l | l | l | l | l | l |}
\hline
N $\diagdown$ BS & 4 & 8 & 16 & 32 & 64  \\ \hline

256 & 18.18 ms & 14.66 ms & \textbf{13.76 ms} & 13.85 ms & 18.35 ms  \\ \hline
512 & 0.153 s & 0.118 s & \textbf{0.112 s} & 0.135 s & 0.144 s  \\ \hline
1024 & 1.248 s & \textbf{1.022 s} & 1.069 s & 1.249 s & 1.414 s  \\ \hline
2048 & 19.54 s & 10.02 s & \textbf{9.907 s} & 11.62 s & 14.36 s  \\ \hline
\end{tabular}
\end{center}

Es ist erkennbar, dass die Bestzeiten mit Blockgrößen von 8 \bzw 16 erreicht werden.
Matrizen dieser Dimension passen auch problemlos in den L1-Cache der eingesetzten CPU
und können daher theoretisch sehr effizient im Rechenprozess verarbeitet werden.
Interessant ist der Fall $N=2048$, die Entscheidung zwischen $BS=4$ und $BS=8$
sorgt für einen Speedup von $\approx 2\times$ ($4\rightarrow8$), die Wahl einer
passenden Blockgröße hat also großen Einfluss auf die Performance dieses Verfahrens.
\newline

Das Blocking\footnote{Das Aufteilen der Gesamtmatrix in kleinere Teilmatrizen} macht für
sehr kleine Stufen ($BS=1,2,4$)  wenig Sinn, die Matrizen sind dann zwar klein, aber
einen Performance-Gewinn erreicht man dadurch nicht. Daher ist es auch nicht besonders sinnvoll,
kleine Matrizen (\zB $32$) überhaupt weiter zu unterteilen, da diese bereits ohne Blocking
im L1-Cache unterkommen können. Wie man auch für $N=64$ gesehen hat \ref{chp:Bench1}, 
entsteht für kleine 
Matrizen durch das Cache-optimierte Verfahren zu viel Overhead und selbst der
Standardalgorithmus läuft schneller.

\section{Zusammenfassung}
Sowohl das Cache-optimierte als auch das parallele Verfahren konnten beide
im Vergleich zum konventionellen Algorithmus teils deutliche Verbesserungen verzeichnen.
Die beiden Verfahren ließen sich sicherlich noch weiter optimieren, \bspw
kann mit AVX-512 die Vektorbreite auf 512 bit erweitert werden, jedoch ist die
CPU-Liste mit AVX-512-Support noch ziemlich kurz. Des Weiteren wird bei den bisherigen Verfahren
die Berechnung nicht auf ungenutzte CPU-Kerne verteilt, denkbar und sinnvoll könnte
deshalb eine Lastverteilung \zB mit OpenMP sein.\newline
 
In Systemen, die mehrere Cache-Level unterstützen, kann das Blocking mehr als $1\times$ angewandt werden,
um das Lokalitätsprinzip in der Speicherverwaltung weiter auszunutzen.
Die Implementierung von ATLAS macht in einer sehr effizienten Weise vom Caching 
und anderen Optimierungsmöglichkeiten\footnote{AEOS} Gebrauch, ein großer Vorteil
liegt hier beim Autotuning -- so wird die Blockgröße und andere Parameter direkt für das 
Ziel-System automatisch optimiert.\newline

Die Feststellung, dass AVX für kleine Matrizen besonders gut geeignet ist und 
Loop tiling für sehr kleine Blockgrößen nicht sinnvoll ist, lässt die Vermutung zu,
dass eine Kombination aus den beiden Verfahren eine weitere Optimierungsmöglichkeit 
bietet. So könnte durch das Blocking die konkrete Matrix auf einen sinnvollen Wert verkleinert
 \ref{chp:BS} und auf den Teilmatrizen dann die parallele Berechnung mit AVX vollzogen werden.


\section{Quellen}
\begin{thebibliography}{9}

\bibitem{ghub} \url{https://github.com/matthiastz/FSP-Matrixmultiplikation}, März 2017
\bibitem{intr} \url{https://software.intel.com/sites/landingpage/IntrinsicsGuide}, März 2017
\bibitem{atlas} \url{http://math-atlas.sourceforge.net}, März 2017
\bibitem{blasqr} \url{http://www.netlib.org/blas/blasqr.pdf}, März 2017

\end{thebibliography}


\end{document}
